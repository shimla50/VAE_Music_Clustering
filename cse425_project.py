# -*- coding: utf-8 -*-
"""cse425_project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10KtJ54TmvvL_3K5-DUmDH_FE_hyZGBN8

Easy Task
"""

from google.colab import files
uploaded = files.upload()  # This will open a file picker

import pandas as pd

df = pd.read_csv("spotify_songs.csv")  # The file will be uploaded to the current directory
df.head()

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv('spotify_songs.csv')  # Update the path if needed

audio_cols = [
    'danceability', 'energy', 'loudness', 'tempo',
    'speechiness', 'acousticness', 'instrumentalness',
    'liveness', 'valence'
]

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler and transform the audio features
audio_features = scaler.fit_transform(df[audio_cols])

# Convert the result back to a DataFrame for easier inspection
audio_features_df = pd.DataFrame(audio_features, columns=audio_cols)

# Show the normalized audio features
audio_features_df.head()

# Check the mean and standard deviation for each feature
print(audio_features_df.mean())  # Should be close to 0
print(audio_features_df.std())   # Should be close to 1

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import numpy as np

# Load the data (replace with actual path to your dataset)
df = pd.read_csv('spotify_songs.csv')

# Fill NaN values in lyrics with an empty string
df['lyrics'] = df['lyrics'].fillna('')

# Preprocess lyrics: remove special characters and keep only alphabets (optional)
df['processed_lyrics'] = df['lyrics'].str.replace('[^a-zA-Z\\s]', '', regex=True)  # Remove non-alphabetic chars

# Initialize the TfidfVectorizer
tfidf = TfidfVectorizer(max_features=5000)

# Fit and transform the lyrics column
lyrics_features = tfidf.fit_transform(df['processed_lyrics']).toarray()

# Check the shape of the feature matrix
print(lyrics_features.shape)  # (number_of_songs, number_of_features)

# Combine with audio features (if already preprocessed)
# Assume 'audio_features' is a numpy array containing audio features
combined_features = np.concatenate([audio_features, lyrics_features], axis=1)

# Now, combined_features can be used for training the VAE model

from sklearn.preprocessing import StandardScaler

# Assume df is your dataframe with audio feature columns
audio_cols = ['loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']
scaler = StandardScaler()
audio_features = scaler.fit_transform(df[audio_cols])  # Now audio_features is a 2D array

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize TF-IDF vectorizer
tfidf = TfidfVectorizer(max_features=5000)  # Set max_features to limit the number of features
lyrics_features = tfidf.fit_transform(df['lyrics']).toarray()  # Transform lyrics to numerical features

import numpy as np

# Concatenate audio features and lyrics features along axis 1 (column-wise)
combined_features = np.concatenate([audio_features, lyrics_features], axis=1)
print("Combined features shape:", combined_features.shape)

pip install tensorflow

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras import backend as K
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Sampling Layer for reparameterization trick
class SamplingLayer(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        batch = K.shape(z_mean)[0]
        dim = K.int_shape(z_mean)[1]
        epsilon = K.random_normal(shape=(batch, dim))
        return z_mean + K.exp(0.5 * z_log_var) * epsilon

# VAE Loss Function: Reconstruction loss + KL divergence
def vae_loss(inputs, decoded, z_mean, z_log_var, input_dim):
    # Reconstruction loss (binary crossentropy)
    reconstruction_loss = tf.keras.losses.binary_crossentropy(inputs, decoded)
    reconstruction_loss *= input_dim  # Scale by input dimension

    # KL divergence loss
    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)

    # Total VAE loss is the sum of reconstruction and KL divergence
    total_loss = K.mean(reconstruction_loss + kl_loss)
    return total_loss

# Build the Encoder
def build_encoder(input_dim, latent_dim):
    inputs = layers.Input(shape=(input_dim,))
    x = layers.Dense(512, activation='relu')(inputs)
    x = layers.Dense(256, activation='relu')(x)

    # Latent space: output mean and log variance
    z_mean = layers.Dense(latent_dim)(x)
    z_log_var = layers.Dense(latent_dim)(x)

    # Sampling layer (reparameterization trick)
    z = SamplingLayer()([z_mean, z_log_var])

    encoder = models.Model(inputs, [z_mean, z_log_var, z])
    return encoder

# Build the Decoder
def build_decoder(latent_dim, input_dim):
    latent_inputs = layers.Input(shape=(latent_dim,))
    x = layers.Dense(256, activation='relu')(latent_inputs)
    x = layers.Dense(512, activation='relu')(x)
    decoded = layers.Dense(input_dim, activation='sigmoid')(x)

    decoder = models.Model(latent_inputs, decoded)
    return decoder

# Build the VAE Model
def build_vae(input_dim, latent_dim):
    # Build encoder and decoder
    encoder = build_encoder(input_dim, latent_dim)
    decoder = build_decoder(latent_dim, input_dim)

    # Define input layer
    inputs = layers.Input(shape=(input_dim,))

    # Encoder output (mean, log variance, and latent vector)
    z_mean, z_log_var, z = encoder(inputs)

    # Reconstruct the input
    reconstructed = decoder(z)

    # Build the VAE model
    vae = models.Model(inputs, reconstructed)

    # Compile the VAE model with custom loss function
    vae.compile(optimizer='adam', loss=lambda x, y: vae_loss(x, y, z_mean, z_log_var, input_dim))

    return vae, encoder, decoder

# Custom training loop to handle loss calculation
def train_vae(vae, encoder, combined_features, epochs=50, batch_size=64):
    # Iterate through the epochs
    for epoch in range(epochs):
        print(f"Epoch {epoch+1}/{epochs}")
        for i in range(0, len(combined_features), batch_size):
            # Get a batch of data
            batch_data = combined_features[i:i+batch_size]

            with tf.GradientTape() as tape:
                # Forward pass: compute the model output
                decoded = vae(batch_data)

                # Compute the loss
                z_mean, z_log_var, _ = encoder(batch_data)
                loss = vae_loss(batch_data, decoded, z_mean, z_log_var, batch_data.shape[1])

            # Get gradients and update the weights
            grads = tape.gradient(loss, vae.trainable_variables)
            vae.optimizer.apply_gradients(zip(grads, vae.trainable_variables))

        print(f"Loss at epoch {epoch+1}: {loss.numpy()}")

# Example of combined_features (audio + lyrics)
# Ensure combined_features is a NumPy array of shape (num_samples, input_dim)
combined_features = np.random.rand(1000, 100)  # Placeholder, replace with actual combined features

# Input dimension and latent space dimension
input_dim = combined_features.shape[1]  # Features length (e.g., number of audio + lyric features)
latent_dim = 2  # You can adjust the size of the latent space

# Build and compile the VAE model
vae, encoder, decoder = build_vae(input_dim, latent_dim)

# Train the VAE model using the custom training loop
train_vae(vae, encoder, combined_features, epochs=50, batch_size=64)

# Once trained, extract latent vectors from the encoder
latent_vectors = encoder.predict(combined_features)[0]

# Apply KMeans clustering on the latent vectors
kmeans = KMeans(n_clusters=5)
clusters = kmeans.fit_predict(latent_vectors)

# Visualize the clustering results using t-SNE
tsne = TSNE(n_components=2, random_state=42)
latent_2d = tsne.fit_transform(latent_vectors)

# Plot the clusters
plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=clusters, cmap='viridis')
plt.colorbar()
plt.title("t-SNE Visualization of Latent Space")
plt.show()

# Evaluate the clustering performance with Silhouette Score
silhouette = silhouette_score(combined_features, clusters)

# Print the clustering performance
print(f"Silhouette Score: {silhouette}")

from sklearn.metrics import silhouette_score, calinski_harabasz_score

# Apply KMeans clustering on the latent vectors from VAE
kmeans = KMeans(n_clusters=5)
clusters = kmeans.fit_predict(latent_vectors)

# Silhouette Score for VAE-based clustering
silhouette_vae = silhouette_score(latent_vectors, clusters)
print(f"Silhouette Score (VAE + K-Means): {silhouette_vae}")

# Calinski-Harabasz Index for VAE-based clustering
calinski_vae = calinski_harabasz_score(latent_vectors, clusters)
print(f"Calinski-Harabasz Index (VAE + K-Means): {calinski_vae}")

# PCA + K-Means for comparison
# Apply PCA to reduce dimensions
pca = PCA(n_components=latent_dim)
pca_features = pca.fit_transform(combined_features)

# Apply KMeans clustering to PCA-reduced features
kmeans_pca = KMeans(n_clusters=5)
clusters_pca = kmeans_pca.fit_predict(pca_features)

# Silhouette Score for PCA + K-Means
silhouette_pca = silhouette_score(combined_features, clusters_pca)
print(f"Silhouette Score (PCA + K-Means): {silhouette_pca}")

# Calinski-Harabasz Index for PCA + K-Means
calinski_pca = calinski_harabasz_score(combined_features, clusters_pca)
print(f"Calinski-Harabasz Index (PCA + K-Means): {calinski_pca}")

# Visualize the clustering results using t-SNE
tsne = TSNE(n_components=2, random_state=42)
latent_2d = tsne.fit_transform(latent_vectors)

# Plot the clusters
plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=clusters, cmap='viridis')
plt.colorbar()
plt.title("t-SNE Visualization of Latent Space (VAE + K-Means)")
plt.show()